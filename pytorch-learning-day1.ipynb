{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Creating tensors and accessing attributes**\n\nTensors are the primary data structure in PyTorch and will be the building blocks for our deep learning models. They share many similarities with NumPy arrays but have some unique attributes too.\n\nIn this exercise, you'll practice creating a tensor from a Python list and displaying some of its attributes.\n\n","metadata":{}},{"cell_type":"code","source":"import torch\n\nlist_a = [1, 2, 3, 4]\nlist_b = [7, 8, 9 ,0]\n# Create a tensor from list_a\ntensor_a = torch.tensor(list_a)\ntensor_b = torch.tensor(list_b)\n\nprint(tensor_a)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-16T16:07:52.356905Z","iopub.execute_input":"2024-05-16T16:07:52.357348Z","iopub.status.idle":"2024-05-16T16:07:52.365684Z","shell.execute_reply.started":"2024-05-16T16:07:52.357321Z","shell.execute_reply":"2024-05-16T16:07:52.364298Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor([1, 2, 3, 4])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display the tensor device\nprint(tensor_a.device)\n\n# Display the tensor data type\nprint(tensor_a.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T16:04:15.575657Z","iopub.execute_input":"2024-05-16T16:04:15.576074Z","iopub.status.idle":"2024-05-16T16:04:15.581938Z","shell.execute_reply.started":"2024-05-16T16:04:15.576028Z","shell.execute_reply":"2024-05-16T16:04:15.580924Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cpu\ntorch.int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Creating tensors from NumPy arrays**\n\nTensors are the fundamental data structure of PyTorch. You can create complex deep learning algorithms by learning how to manipulate them.\n\nThe torch package has been imported, and two NumPy arrays have been created, named array_a and array_b. Both arrays have the same dimensions.","metadata":{}},{"cell_type":"code","source":"array_a = torch.tensor(list_a)\narray_b = torch.tensor(list_b)\n\n# Create two tensors from the arrays\ntensor_a = torch.tensor(array_a)\ntensor_b = torch.tensor(array_b)\n\n# Subtract tensor_b from tensor_a \ntensor_c = tensor_a - tensor_b\nprint(tensor_c)\n\n# Multiply each element of tensor_a with each element of tensor_b\ntensor_d = tensor_a * tensor_b\nprint(tensor_d)\n\n# Add tensor_c with tensor_d\ntensor_e = tensor_c + tensor_d\nprint(tensor_e)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T16:10:05.528091Z","iopub.execute_input":"2024-05-16T16:10:05.528525Z","iopub.status.idle":"2024-05-16T16:10:05.539471Z","shell.execute_reply.started":"2024-05-16T16:10:05.528494Z","shell.execute_reply":"2024-05-16T16:10:05.538147Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([-6, -6, -6,  4])\ntensor([ 7, 16, 27,  0])\ntensor([ 1, 10, 21,  4])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/1309791693.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  tensor_a = torch.tensor(array_a)\n/tmp/ipykernel_33/1309791693.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  tensor_b = torch.tensor(array_b)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**First neural network**\n\nWe will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor.\n\nCreate a neural network of linear layers that takes a tensor of dimensions 1*8\n as input and outputs a tensor of dimensions  1*1\n.\nUse any output dimension for the first layer you want.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ninput_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n\n# Implement a small neural network with exactly two linear layers\nmodel = nn.Sequential( nn.Linear(8,16), # Linear layer with input size 8 and output size 16\n                       nn.Linear(16,1)   # Linear layer with input size 16 and output size 1\n\n                     )\n\noutput = model(input_tensor)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T16:35:42.133652Z","iopub.execute_input":"2024-05-16T16:35:42.134325Z","iopub.status.idle":"2024-05-16T16:35:42.196976Z","shell.execute_reply.started":"2024-05-16T16:35:42.134274Z","shell.execute_reply":"2024-05-16T16:35:42.195976Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[0.1093]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":" In practice, you'll find that modern neural networks can contain hundreds of layers and millions of parameters. Recall that the model output is not meaningful until the model is trained, i.e. until the weights and biases of each layer can meaningfully be used to produce output.","metadata":{}},{"cell_type":"code","source":"input_tensor = torch.Tensor(\n    [[2, 3, 6, 7, 9, 3, 2, 1, 5, 3, 6, 9]]\n    )\n\nnn.Sequential(\nnn.Linear(12, 20),\nnn.Linear(20,14),\nnn.Linear(14, 3),\nnn.Linear(3, 2)\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T16:40:16.759050Z","iopub.execute_input":"2024-05-16T16:40:16.759456Z","iopub.status.idle":"2024-05-16T16:40:16.771451Z","shell.execute_reply.started":"2024-05-16T16:40:16.759427Z","shell.execute_reply":"2024-05-16T16:40:16.770254Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Linear(in_features=12, out_features=20, bias=True)\n  (1): Linear(in_features=20, out_features=14, bias=True)\n  (2): Linear(in_features=14, out_features=3, bias=True)\n  (3): Linear(in_features=3, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# The sigmoid and softmax functions\n\nThe sigmoid and softmax functions are two of the most popular activation functions in deep learning. They are both usually used as the last step of a neural network. Sigmoid functions are used for binary classification problems, whereas softmax functions are often used for multi-class classification problems. This exercise will familiarize you with creating and using both functions.\n\nLet's say that you have a neural network that returned the values contained in the score tensor as a pre-activation output. You will apply activation functions to this output.\n","metadata":{}},{"cell_type":"code","source":"# Create a sigmoid function and apply it on input_tensor to generate a probability.\ninput_tensor = torch.tensor([[0.8]])\n\n# Create a sigmoid function and apply it on input_tensor\nsigmoid = nn.Sigmoid()\nprobability = sigmoid(input_tensor)\nprint(probability)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T16:56:09.957279Z","iopub.execute_input":"2024-05-16T16:56:09.957763Z","iopub.status.idle":"2024-05-16T16:56:09.967099Z","shell.execute_reply.started":"2024-05-16T16:56:09.957730Z","shell.execute_reply":"2024-05-16T16:56:09.965747Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tensor([[0.6900]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a softmax function and apply it on input_tensor to generate a probability.\n\ninput_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n\n# Create a softmax function and apply it on input_tensor\nsoftmax = nn.Softmax(dim = -1)\nprobabilities = softmax(input_tensor)\nprint(probabilities)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T16:57:53.804535Z","iopub.execute_input":"2024-05-16T16:57:53.804994Z","iopub.status.idle":"2024-05-16T16:57:53.814120Z","shell.execute_reply.started":"2024-05-16T16:57:53.804964Z","shell.execute_reply":"2024-05-16T16:57:53.812764Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building a binary classifier in PyTorch\n\nRecall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n\nIn this exercise, you'll practice building this small network and interpreting the output of the classifier.\n\nCreate a neural network that takes a tensor of dimensions 1x8 as input, and returns an output of the correct shape for binary classification.\nPass the output of the linear layer to a sigmoid, which both takes in and return a single float.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ninput_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n\n# Implement a small neural network for binary classification\nmodel = nn.Sequential(\n  nn.Linear(8,1),\n  nn.Sigmoid()\n)\n\noutput = model(input_tensor)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:33:37.545167Z","iopub.execute_input":"2024-05-16T17:33:37.546109Z","iopub.status.idle":"2024-05-16T17:33:37.556707Z","shell.execute_reply.started":"2024-05-16T17:33:37.546040Z","shell.execute_reply":"2024-05-16T17:33:37.555243Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"tensor([[0.0719]], grad_fn=<SigmoidBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# From regression to multi-class classification\n\nRecall that the models we have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n\nIn this exercise, you'll start by building a model for regression, and then tweak the model to perform a multi-class classification.","metadata":{}},{"cell_type":"markdown","source":"Create a neural network with exactly four linear layers, which takes the input tensor as input, and outputs a regression value, using any shapes you like for the hidden layers.\n\n","metadata":{}},{"cell_type":"code","source":"input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n\n# Implement a neural network with exactly four linear layers\nmodel = nn.Sequential(nn.Linear(11,8), nn.Linear(8,16),\n                      nn.Linear(16,32), nn.Linear(32,1))\n\noutput = model(input_tensor)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:42:08.134797Z","iopub.execute_input":"2024-05-16T17:42:08.135198Z","iopub.status.idle":"2024-05-16T17:42:08.144965Z","shell.execute_reply.started":"2024-05-16T17:42:08.135170Z","shell.execute_reply":"2024-05-16T17:42:08.143802Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor([[0.0655]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A similar neural network to the one you just built is provided, containing four linear layers; update this network to perform a multi-class classification with four outputs.","metadata":{}},{"cell_type":"code","source":"\ninput_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n\nmodel = nn.Sequential(\n    nn.Linear(11, 20),\n    nn.ReLU(),  # Adding ReLU activation after the first layer\n    nn.Linear(20, 12),\n    nn.ReLU(),  # Adding ReLU activation after the second layer\n    nn.Linear(12, 6),\n    nn.ReLU(),  # Adding ReLU activation after the third layer\n    nn.Linear(6, 4),  # Output layer with 4 units for multi-class classification\n    nn.Softmax(dim=1)  # Softmax activation to get probabilities for each class\n)\n\n\noutput = model(input_tensor)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:46:14.546361Z","iopub.execute_input":"2024-05-16T17:46:14.546798Z","iopub.status.idle":"2024-05-16T17:46:14.564077Z","shell.execute_reply.started":"2024-05-16T17:46:14.546767Z","shell.execute_reply":"2024-05-16T17:46:14.562276Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"tensor([[0.2124, 0.1997, 0.1948, 0.3932]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You turned your continuous regression values into probabilities bounded between zero and one by changing the output dimensions of the last linear layer, as well as by applying the softmax function.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}