{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOmuXExBD3/llk4JaWZis9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushanttwayana/Learning-Machine-Learning/blob/main/DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zk4mrAR-p2N"
      },
      "outputs": [],
      "source": [
        "# Create music_dummies\n",
        "music_dummies =pd.get_dummies(music_df,drop_first = True)\n",
        "\n",
        "# Print the new DataFrame's shape\n",
        "print(\"Shape of music_dummies: {}\".format(music_dummies.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape of music_dummies: (1000, 20)\n",
        "\n",
        "As there were ten values in the \"genre\" column, nine new columns were added by a call of pd.get_dummies() using drop_first=True. After dropping the original \"genre\" column, there are still eight new columns in the DataFrame!"
      ],
      "metadata": {
        "id": "1DqUSVxf_Dzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X and y\n",
        "X = music_dummies.drop(\"popularity\",axis = 1).values\n",
        "y = music_dummies[\"popularity\"].values\n",
        "\n",
        "# Instantiate a ridge model\n",
        "ridge = Ridge(alpha = 0.2)\n",
        "\n",
        "# X_train, X_test , y_train, y_test = train_test_split(X, y,test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(-scores)\n",
        "print(\"Average RMSE: {}\".format(np.mean(rmse)))\n",
        "print(\"Standard Deviation of the target array: {}\".format(np.std(y)))"
      ],
      "metadata": {
        "id": "Ms83GuI2_Gwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<script.py> output:\n",
        "    Average RMSE: 8.236853840202299\n",
        "    Standard Deviation of the target array: 14.02156909907019\n",
        "\n",
        "    An average RMSE of approximately 8.24 is lower than the standard deviation of the target variable (song popularity), suggesting the model is reasonably accurate.\n",
        "\n"
      ],
      "metadata": {
        "id": "NwjArJnyGDsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression with categorical features\n",
        "Now you have created music_dummies, containing binary features for each song's genre, it's time to build a ridge regression model to predict song popularity.\n",
        "\n",
        "music_dummies has been preloaded for you, along with Ridge, cross_val_score, numpy as np, and a KFold object stored as kf.\n",
        "\n",
        "The model will be evaluated by calculating the average RMSE, but first, you will need to convert the scores for each fold to positive values and take their square root. This metric shows the average error of our model's predictions, so it can be compared against the standard deviation of the target value—\"popularity\".\n",
        "\n",
        "Create X, containing all features in music_dummies, and y, consisting of the \"popularity\" column, respectively.\n",
        "\n",
        "Instantiate a ridge regression model, setting alpha equal to 0.2.\n",
        "\n",
        "Perform cross-validation on X and y using the ridge model, setting cv equal to kf, and using negative mean squared error as the scoring metric.\n",
        "\n",
        "Print the RMSE values by converting negative scores to positive and taking the square root.\n",
        "\n"
      ],
      "metadata": {
        "id": "fj9tCx06FcRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**MISSING DATA HANDLING**\n",
        "\n"
      ],
      "metadata": {
        "id": "7b-_tLeOLioA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping missing data\n",
        "Over the next three exercises, you are going to tidy the music_df dataset. You will create a pipeline to impute missing values and build a KNN classifier model, then use it to predict whether a song is of the \"Rock\" genre.\n",
        "\n",
        "In this exercise specifically, you will drop missing values accounting for less than 5% of the dataset, and convert the \"genre\" column into a binary feature."
      ],
      "metadata": {
        "id": "0s6TXw5wLwmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of missing values for each column in the music_df dataset, sorted in ascending order.\n",
        "\n",
        "print(music_df.isna().sum().sort_values())"
      ],
      "metadata": {
        "id": "1Mkd7K3iGJ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "genre                 8\n",
        "\n",
        "popularity           31\n",
        "\n",
        "loudness             44\n",
        "\n",
        "liveness             46\n",
        "\n",
        "tempo                46\n",
        "\n",
        "speechiness          59\n",
        "\n",
        "duration_ms          91\n",
        "\n",
        "instrumentalness     91\n",
        "\n",
        "danceability        143\n",
        "\n",
        "valence             143\n",
        "\n",
        "acousticness        200\n",
        "\n",
        "energy              200\n",
        "\n",
        "dtype: int64"
      ],
      "metadata": {
        "id": "uaEZeV1VL81h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove values where less than 5% are missing\n",
        "music_df = music_df.dropna(subset=[\"genre\", \"popularity\",\"loudness\", \"liveness\", \"tempo\"])\n",
        "\n",
        "print(music_df.isna().sum().sort_values())"
      ],
      "metadata": {
        "id": "hUamsmoHL9cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "popularity            0\n",
        "\n",
        "liveness              0\n",
        "\n",
        "loudness              0\n",
        "\n",
        "tempo                 0\n",
        "\n",
        "genre                 0\n",
        "\n",
        "duration_ms          29\n",
        "\n",
        "instrumentalness     29\n",
        "\n",
        "speechiness          53\n",
        "\n",
        "danceability        127\n",
        "\n",
        "valence             127\n",
        "\n",
        "acousticness        178\n",
        "\n",
        "energy              178\n",
        "\n",
        "dtype: int64"
      ],
      "metadata": {
        "id": "JxTm6nFWMlQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print missing values for each column\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "# Remove values where less than 5% are missing\n",
        "music_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n",
        "\n",
        "# Convert music_df[\"genre\"] to values of 1 if the row contains \"Rock\", otherwise change the value to 0.\n",
        "# Convert genre to a binary feature\n",
        "music_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n",
        "\n",
        "print(music_df.isna().sum().sort_values())\n",
        "print(\"Shape of the `music_df`: {}\".format(music_df.shape))"
      ],
      "metadata": {
        "id": "Bfgl4it3Mo3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "popularity            0\n",
        "\n",
        "liveness              0\n",
        "\n",
        "loudness              0\n",
        "\n",
        "tempo                 0\n",
        "\n",
        "genre                 0\n",
        "\n",
        "duration_ms          29\n",
        "\n",
        "instrumentalness     29\n",
        "\n",
        "speechiness          53\n",
        "\n",
        "danceability        127\n",
        "\n",
        "valence             127\n",
        "\n",
        "acousticness        178\n",
        "\n",
        "energy              178\n",
        "\n",
        "dtype: int64\n",
        "\n",
        "Shape of the `music_df`: (892, 12)"
      ],
      "metadata": {
        "id": "e8xqv7mzNfpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has gone from 1000 observations down to 892, but it is now in the correct format for binary classification and the remaining missing values can be imputed as part of a pipeline.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UOyuY57PNkvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Pipeline for song genre prediction:**\n",
        "\n",
        "Now it's time to build a pipeline. It will contain steps to impute missing values using the mean for each feature and build a KNN model for the classification of song genre.\n",
        "\n",
        "The modified music_df dataset that you created in the previous exercise has been preloaded for you, along with KNeighborsClassifier and train_test_split.\n",
        "\n",
        "Import SimpleImputer and Pipeline.\n",
        "\n",
        "Instantiate an imputer.\n",
        "\n",
        "Instantiate a KNN classifier with three neighbors.\n",
        "\n",
        "Create steps, a list of tuples containing the imputer variable you created, called \"imputer\", followed by the knn model you created, called \"knn\"."
      ],
      "metadata": {
        "id": "LtZIGbyAPaKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Instantiate an imputer\n",
        "imputer = SimpleImputer()\n",
        "\n",
        "# Instantiate a knn model\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "\n",
        "# Build steps for the pipeline\n",
        "steps = [(\"imputer\", imputer),\n",
        "         (\"knn\", knn)]\n",
        "\n",
        "pipeline = Pipeline(steps)"
      ],
      "metadata": {
        "id": "HJF1sjLoNkHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pipeline for song genre prediction: II*\n",
        "\n",
        "Having set up the steps of the pipeline in the previous exercise, you will now use it on the music_df dataset to classify the genre of songs. What makes pipelines so incredibly useful is the simple interface that they provide.\n",
        "\n",
        "X_train, X_test, y_train, and y_test have been preloaded for you, and confusion_matrix has been imported from sklearn.metrics.\n",
        "\n",
        "Create a pipeline using the steps you previously defined.\n",
        "\n",
        "Fit the pipeline to the training data.\n",
        "\n",
        "Make predictions on the test set.\n",
        "\n",
        "Calculate and print the confusion matrix."
      ],
      "metadata": {
        "id": "vJi27VocSW0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [(\"imputer\", imp_mean),\n",
        "        (\"knn\", knn)]\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "YqJXiCGtSVuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<script.py> output:\n",
        "    [[79  9]\n",
        "     [ 4 82]]\n",
        "\n",
        "     See how easy it is to scale our model building workflow using pipelines. In this case, the confusion matrix highlights that the model had 79 true positives and 82 true negatives!"
      ],
      "metadata": {
        "id": "7P5-SkgSSYsa"
      }
    }
  ]
}